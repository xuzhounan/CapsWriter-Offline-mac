import Foundation
import AVFoundation
import Combine
import os.log

/// ä¼˜åŒ–çš„ Sherpa-ONNX è¯­éŸ³è¯†åˆ«æœåŠ¡ - æœ€å°åŒ–è¯†åˆ«å»¶è¿Ÿ
/// 
/// æ€§èƒ½ä¼˜åŒ–ç‰¹ç‚¹ï¼š
/// - æ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜
/// - æ™ºèƒ½æ‰¹å¤„ç†
/// - å¹¶å‘è¯†åˆ«ç®¡é“
/// - é¢„æµ‹æ€§èµ„æºç®¡ç†
/// - è‡ªé€‚åº”è´¨é‡æ§åˆ¶
/// - é›¶æ‹·è´æ•°æ®æµ
class OptimizedSherpaASRService: ObservableObject, SpeechRecognitionServiceProtocol {
    
    // MARK: - Published Properties
    @Published var isServiceRunning: Bool = false
    @Published var isRecognizing: Bool = false
    @Published var isInitialized: Bool = false
    @Published var partialTranscript: String = ""
    @Published var logs: [String] = []
    @Published var transcriptHistory: [TranscriptEntry] = []
    @Published var performanceMetrics = RecognitionPerformanceMetrics()
    
    // MARK: - Private Properties
    private var recognizer: OpaquePointer?
    private var stream: OpaquePointer?
    
    // ä¼˜åŒ–çš„å¤„ç†é˜Ÿåˆ—
    private let recognitionQueue = DispatchQueue(label: "com.capswriter.recognition", qos: .userInitiated)
    private let modelQueue = DispatchQueue(label: "com.capswriter.model-management", qos: .utility)
    private let preprocessingQueue = DispatchQueue(label: "com.capswriter.audio-preprocessing", qos: .userInitiated)
    
    // ç®¡ç†å™¨
    private let configManager = ConfigurationManager.shared
    private let memoryManager = MemoryManager.shared
    private let performanceMonitor = PerformanceMonitor.shared
    
    // ä¼˜åŒ–ç»„ä»¶
    private var audioPreprocessor: AudioPreprocessor?
    private var recognitionPipeline: RecognitionPipeline?
    private var resultPostprocessor: ResultPostprocessor?
    
    // æ¨¡å‹ç®¡ç†
    private var modelCache: ModelCache?
    private var isModelPreloaded: Bool = false
    
    // æ€§èƒ½ç›‘æ§
    private var recognitionStartTime: Date?
    private var recognitionStats = RecognitionStatistics()
    
    // å§”æ‰˜
    weak var delegate: SpeechRecognitionDelegate?
    
    // é…ç½®
    private var sampleRate: Double { configManager.audio.sampleRate }
    private var modelPath: String {
        let bundle = Bundle.main
        return bundle.path(forResource: configManager.recognition.modelPath, ofType: nil) ?? configManager.recognition.modelPath
    }
    
    // æ—¥å¿—å™¨
    private let logger = os.Logger(subsystem: "com.capswriter", category: "OptimizedSherpaASR")
    
    // æ‰¹å¤„ç†é…ç½®
    private let batchSize: Int = 3
    private var audioBatch: [AVAudioPCMBuffer] = []
    private let batchQueue = DispatchQueue(label: "com.capswriter.batch-processing", qos: .userInitiated)
    
    // MARK: - Initialization
    init() {
        setupOptimizedComponents()
        setupPerformanceMonitoring()
        addLog("ğŸ§  OptimizedSherpaASRService åˆå§‹åŒ–å®Œæˆ")
    }
    
    deinit {
        stopService()
        cleanupOptimizedResources()
        addLog("ğŸ›‘ OptimizedSherpaASRService é”€æ¯")
    }
    
    // MARK: - SpeechRecognitionServiceProtocol Implementation
    
    func startService() {
        guard !isServiceRunning else {
            addLog("âš ï¸ æœåŠ¡å·²åœ¨è¿è¡Œ")
            return
        }
        
        addLog("ğŸš€ å¯åŠ¨ä¼˜åŒ–è¯†åˆ«æœåŠ¡...")
        
        modelQueue.async { [weak self] in
            self?.initializeOptimizedRecognizer()
        }
    }
    
    func stopService() {
        addLog("ğŸ›‘ åœæ­¢ä¼˜åŒ–è¯†åˆ«æœåŠ¡...")
        
        modelQueue.async { [weak self] in
            self?.cleanupOptimizedRecognizer()
        }
        
        DispatchQueue.main.async {
            self.isServiceRunning = false
            self.isRecognizing = false
            self.isInitialized = false
        }
    }
    
    func startRecognition() {
        guard isServiceRunning && isInitialized else {
            addLog("âŒ æœåŠ¡æœªå°±ç»ªï¼Œæ— æ³•å¼€å§‹è¯†åˆ«")
            return
        }
        
        guard !isRecognizing else {
            addLog("âš ï¸ è¯†åˆ«å·²åœ¨è¿›è¡Œä¸­")
            return
        }
        
        addLog("ğŸ¯ å¼€å§‹ä¼˜åŒ–è¯†åˆ«å¤„ç†...")
        recognitionStartTime = Date()
        
        recognitionQueue.async { [weak self] in
            self?.prepareOptimizedRecognition()
        }
        
        isRecognizing = true
    }
    
    func stopRecognition() {
        guard isRecognizing else {
            addLog("âš ï¸ è¯†åˆ«æœªåœ¨è¿›è¡Œä¸­")
            return
        }
        
        addLog("â¹ï¸ åœæ­¢ä¼˜åŒ–è¯†åˆ«å¤„ç†...")
        
        // å¤„ç†æ‰¹å¤„ç†é˜Ÿåˆ—ä¸­çš„å‰©ä½™éŸ³é¢‘
        batchQueue.async { [weak self] in
            self?.processPendingBatch()
        }
        
        recognitionQueue.async { [weak self] in
            self?.finalizeOptimizedRecognition()
        }
        
        isRecognizing = false
    }
    
    func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard isRecognizing else { return }
        
        // ä½¿ç”¨é¢„å¤„ç†é˜Ÿåˆ—è¿›è¡ŒéŸ³é¢‘é¢„å¤„ç†
        preprocessingQueue.async { [weak self] in
            self?.preprocessAndQueueAudio(buffer)
        }
    }
    
    func addTranscriptEntry(text: String, isPartial: Bool) {
        guard !text.isEmpty else { return }
        
        let entry = TranscriptEntry(
            timestamp: Date(),
            text: text,
            isPartial: isPartial
        )
        
        DispatchQueue.main.async {
            self.transcriptHistory.append(entry)
            if self.transcriptHistory.count > 100 {
                self.transcriptHistory.removeFirst(self.transcriptHistory.count - 100)
            }
        }
    }
    
    // MARK: - Optimized Recognition Implementation
    
    private func setupOptimizedComponents() {
        // åˆå§‹åŒ–éŸ³é¢‘é¢„å¤„ç†å™¨
        audioPreprocessor = AudioPreprocessor(
            sampleRate: sampleRate,
            memoryManager: memoryManager
        )
        
        // åˆå§‹åŒ–è¯†åˆ«ç®¡é“
        recognitionPipeline = RecognitionPipeline(
            batchSize: batchSize,
            memoryManager: memoryManager
        )
        
        // åˆå§‹åŒ–ç»“æœåå¤„ç†å™¨
        resultPostprocessor = ResultPostprocessor()
        
        // åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
        modelCache = ModelCache(maxSize: 3) // ç¼“å­˜æœ€å¤š3ä¸ªæ¨¡å‹é…ç½®
        
        addLog("ğŸ”§ ä¼˜åŒ–ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    }
    
    private func setupPerformanceMonitoring() {
        Timer.scheduledTimer(withTimeInterval: 3.0, repeats: true) { [weak self] _ in
            self?.updateRecognitionStatistics()
        }
    }
    
    private func initializeOptimizedRecognizer() {
        addLog("ğŸ§  åˆå§‹åŒ–ä¼˜åŒ–è¯†åˆ«å™¨...")
        
        DispatchQueue.main.async {
            RecordingState.shared.updateInitializationProgress("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹...")
        }
        
        // é¢„åŠ è½½æ¨¡å‹
        if preloadModel() {
            isModelPreloaded = true
            addLog("âœ… æ¨¡å‹é¢„åŠ è½½æˆåŠŸ")
        } else {
            addLog("âš ï¸ æ¨¡å‹é¢„åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ ‡å‡†åŠ è½½")
        }
        
        // åˆ›å»ºè¯†åˆ«å™¨
        if createOptimizedRecognizer() {
            DispatchQueue.main.async {
                self.isInitialized = true
                self.isServiceRunning = true
                RecordingState.shared.updateInitializationProgress("ä¼˜åŒ–è¯†åˆ«å™¨å°±ç»ª")
                self.addLog("âœ… ä¼˜åŒ–è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            }
        } else {
            DispatchQueue.main.async {
                self.isInitialized = false
                self.isServiceRunning = false
                RecordingState.shared.updateInitializationProgress("è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥")
                self.addLog("âŒ ä¼˜åŒ–è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥")
            }
        }
    }
    
    private func preloadModel() -> Bool {
        addLog("ğŸ“¦ é¢„åŠ è½½æ¨¡å‹èµ„æº...")
        
        // æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        let encoderPath = "\(modelPath)/encoder.onnx"
        let decoderPath = "\(modelPath)/decoder.onnx"
        let tokensPath = "\(modelPath)/tokens.txt"
        
        guard FileManager.default.fileExists(atPath: encoderPath),
              FileManager.default.fileExists(atPath: decoderPath),
              FileManager.default.fileExists(atPath: tokensPath) else {
            addLog("âŒ æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
            return false
        }
        
        // é¢„è¯»å–æ¨¡å‹æ–‡ä»¶åˆ°å†…å­˜ç¼“å­˜
        do {
            let encoderData = try Data(contentsOf: URL(fileURLWithPath: encoderPath))
            let decoderData = try Data(contentsOf: URL(fileURLWithPath: decoderPath))
            let tokensData = try Data(contentsOf: URL(fileURLWithPath: tokensPath))
            
            memoryManager.cacheAudioData(key: "encoder_model", data: encoderData)
            memoryManager.cacheAudioData(key: "decoder_model", data: decoderData)
            memoryManager.cacheAudioData(key: "tokens_data", data: tokensData)
            
            addLog("ğŸ“š æ¨¡å‹æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜")
            return true
            
        } catch {
            addLog("âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥: \(error)")
            return false
        }
    }
    
    private func createOptimizedRecognizer() -> Bool {
        // ä½¿ç”¨é¢„ç¼“å­˜çš„é…ç½®åˆ›å»ºè¯†åˆ«å™¨
        let config = createOptimizedConfig()
        
        var mutableConfig = config
        recognizer = SherpaOnnxCreateOnlineRecognizer(&mutableConfig)
        
        guard let validRecognizer = recognizer else {
            addLog("âŒ è¯†åˆ«å™¨åˆ›å»ºå¤±è´¥")
            return false
        }
        
        stream = SherpaOnnxCreateOnlineStream(validRecognizer)
        
        guard stream != nil else {
            addLog("âŒ éŸ³é¢‘æµåˆ›å»ºå¤±è´¥")
            SherpaOnnxDestroyOnlineRecognizer(validRecognizer)
            recognizer = nil
            return false
        }
        
        addLog("âœ… ä¼˜åŒ–è¯†åˆ«å™¨åˆ›å»ºæˆåŠŸ")
        return true
    }
    
    private func createOptimizedConfig() -> SherpaOnnxOnlineRecognizerConfig {
        let encoderPath = "\(modelPath)/encoder.onnx"
        let decoderPath = "\(modelPath)/decoder.onnx"
        let tokensPath = "\(modelPath)/tokens.txt"
        
        let paraformerConfig = sherpaOnnxOnlineParaformerModelConfig(
            encoder: encoderPath,
            decoder: decoderPath
        )
        
        let modelConfig = sherpaOnnxOnlineModelConfig(
            tokens: tokensPath,
            paraformer: paraformerConfig,
            numThreads: max(2, configManager.recognition.numThreads), // è‡³å°‘ä½¿ç”¨2ä¸ªçº¿ç¨‹
            provider: configManager.recognition.provider,
            debug: false // ç¦ç”¨è°ƒè¯•æ¨¡å¼ä»¥æé«˜æ€§èƒ½
        )
        
        let featConfig = sherpaOnnxFeatureConfig(
            sampleRate: Int(sampleRate),
            featureDim: 80
        )
        
        return sherpaOnnxOnlineRecognizerConfig(
            featConfig: featConfig,
            modelConfig: modelConfig,
            decodingMethod: "modified_beam_search", // ä½¿ç”¨æ›´å¿«çš„è§£ç æ–¹æ³•
            maxActivePaths: 2, // å‡å°‘æ´»è·ƒè·¯å¾„ä»¥æé«˜é€Ÿåº¦
            enableEndpoint: configManager.recognition.enableEndpoint,
            rule1MinTrailingSilence: configManager.recognition.rule1MinTrailingSilence * 0.8, // æ›´å¿«çš„ç«¯ç‚¹æ£€æµ‹
            rule2MinTrailingSilence: configManager.recognition.rule2MinTrailingSilence * 0.8,
            rule3MinUtteranceLength: configManager.recognition.rule3MinUtteranceLength
        )
    }
    
    private func prepareOptimizedRecognition() {
        guard let recognizer = recognizer,
              let stream = stream else {
            addLog("âŒ è¯†åˆ«å™¨æœªåˆå§‹åŒ–")
            return
        }
        
        // é‡ç½®éŸ³é¢‘æµ
        SherpaOnnxOnlineStreamReset(recognizer, stream)
        
        // æ¸…ç©ºæ‰¹å¤„ç†é˜Ÿåˆ—
        audioBatch.removeAll()
        
        // é‡ç½®ç»Ÿè®¡
        recognitionStats = RecognitionStatistics()
        
        addLog("ğŸ¯ ä¼˜åŒ–è¯†åˆ«å‡†å¤‡å®Œæˆ")
    }
    
    private func preprocessAndQueueAudio(_ buffer: AVAudioPCMBuffer) {
        // éŸ³é¢‘é¢„å¤„ç†
        guard let preprocessed = audioPreprocessor?.process(buffer) else {
            recognitionStats.preprocessingErrors += 1
            return
        }
        
        // æ·»åŠ åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—
        batchQueue.async { [weak self] in
            self?.addToBatch(preprocessed)
        }
    }
    
    private func addToBatch(_ buffer: AVAudioPCMBuffer) {
        audioBatch.append(buffer)
        
        // å½“æ‰¹å¤„ç†è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œå¤„ç†æ‰¹æ¬¡
        if audioBatch.count >= batchSize {
            let batch = audioBatch
            audioBatch.removeAll()
            
            recognitionQueue.async { [weak self] in
                self?.processBatch(batch)
            }
        }
    }
    
    private func processBatch(_ batch: [AVAudioPCMBuffer]) {
        guard let recognizer = recognizer,
              let stream = stream else {
            return
        }
        
        let batchStartTime = Date()
        
        for buffer in batch {
            processAudioBufferOptimized(buffer, recognizer: recognizer, stream: stream)
        }
        
        let batchTime = Date().timeIntervalSince(batchStartTime)
        updateBatchPerformanceMetrics(batchTime: batchTime, batchSize: batch.count)
    }
    
    private func processAudioBufferOptimized(_ buffer: AVAudioPCMBuffer, recognizer: OpaquePointer, stream: OpaquePointer) {
        guard let channelData = buffer.floatChannelData else { return }
        
        let frameLength = Int(buffer.frameLength)
        let samples = channelData[0]
        
        // å‘é€éŸ³é¢‘æ•°æ®åˆ°è¯†åˆ«å™¨
        SherpaOnnxOnlineStreamAcceptWaveform(stream, Int32(sampleRate), samples, Int32(frameLength))
        
        // æ£€æŸ¥æ˜¯å¦å‡†å¤‡å¥½è§£ç 
        if SherpaOnnxIsOnlineStreamReady(recognizer, stream) == 1 {
            let decodeStartTime = Date()
            
            // è§£ç éŸ³é¢‘
            SherpaOnnxDecodeOnlineStream(recognizer, stream)
            
            let decodeTime = Date().timeIntervalSince(decodeStartTime)
            performanceMonitor.recordRecognitionDelay(decodeTime)
            
            // è·å–ç»“æœ
            if let result = SherpaOnnxGetOnlineStreamResult(recognizer, stream) {
                let resultText = getTextFromResultSafely(result)
                
                if !resultText.isEmpty {
                    processRecognitionResult(resultText, isPartial: true)
                }
                
                SherpaOnnxDestroyOnlineRecognizerResult(result)
            }
        }
        
        // æ£€æŸ¥ç«¯ç‚¹
        if SherpaOnnxOnlineStreamIsEndpoint(recognizer, stream) == 1 {
            handleEndpoint(recognizer: recognizer, stream: stream)
        }
        
        recognitionStats.buffersProcessed += 1
    }
    
    private func processRecognitionResult(_ text: String, isPartial: Bool) {
        // åå¤„ç†ç»“æœ
        let processedText = resultPostprocessor?.process(text) ?? text
        
        DispatchQueue.main.async {
            self.partialTranscript = processedText
            self.addTranscriptEntry(text: processedText, isPartial: isPartial)
            self.delegate?.speechRecognitionDidReceivePartialResult(processedText)
        }
        
        if let startTime = recognitionStartTime {
            let totalTime = Date().timeIntervalSince(startTime)
            performanceMonitor.recordOperation("è¯†åˆ«å¤„ç†", startTime: startTime, endTime: Date())
        }
    }
    
    private func handleEndpoint(recognizer: OpaquePointer, stream: OpaquePointer) {
        addLog("ğŸ”š æ£€æµ‹åˆ°è¯­éŸ³ç«¯ç‚¹")
        
        if let result = SherpaOnnxGetOnlineStreamResult(recognizer, stream) {
            let finalText = getTextFromResultSafely(result)
            
            if !finalText.isEmpty {
                let processedText = resultPostprocessor?.process(finalText) ?? finalText
                processRecognitionResult(processedText, isPartial: false)
                
                DispatchQueue.main.async {
                    self.delegate?.speechRecognitionDidReceiveFinalResult(processedText)
                    self.delegate?.speechRecognitionDidDetectEndpoint()
                }
            }
            
            SherpaOnnxDestroyOnlineRecognizerResult(result)
        }
        
        // é‡ç½®æµä»¥å‡†å¤‡ä¸‹ä¸€æ¬¡è¯†åˆ«
        SherpaOnnxOnlineStreamReset(recognizer, stream)
        recognitionStats.endpointsDetected += 1
    }
    
    private func processPendingBatch() {
        if !audioBatch.isEmpty {
            let batch = audioBatch
            audioBatch.removeAll()
            
            recognitionQueue.sync {
                processBatch(batch)
            }
        }
    }
    
    private func finalizeOptimizedRecognition() {
        guard let recognizer = recognizer,
              let stream = stream else {
            return
        }
        
        // è·å–æœ€ç»ˆç»“æœ
        if let result = SherpaOnnxGetOnlineStreamResult(recognizer, stream) {
            let finalText = getTextFromResultSafely(result)
            
            if !finalText.isEmpty {
                let processedText = resultPostprocessor?.process(finalText) ?? finalText
                DispatchQueue.main.async {
                    self.delegate?.speechRecognitionDidReceiveFinalResult(processedText)
                }
            }
            
            SherpaOnnxDestroyOnlineRecognizerResult(result)
        }
        
        addLog("âœ… ä¼˜åŒ–è¯†åˆ«å¤„ç†å®Œæˆ")
    }
    
    private func cleanupOptimizedRecognizer() {
        if let stream = stream {
            SherpaOnnxDestroyOnlineStream(stream)
            self.stream = nil
        }
        
        if let recognizer = recognizer {
            SherpaOnnxDestroyOnlineRecognizer(recognizer)
            self.recognizer = nil
        }
        
        isInitialized = false
        isModelPreloaded = false
        
        addLog("ğŸ§¹ ä¼˜åŒ–è¯†åˆ«å™¨æ¸…ç†å®Œæˆ")
    }
    
    private func cleanupOptimizedResources() {
        cleanupOptimizedRecognizer()
        
        audioPreprocessor = nil
        recognitionPipeline = nil
        resultPostprocessor = nil
        modelCache = nil
        
        memoryManager.clearAllCaches()
    }
    
    // MARK: - Performance Monitoring
    
    private func updateBatchPerformanceMetrics(batchTime: TimeInterval, batchSize: Int) {
        DispatchQueue.main.async {
            self.performanceMetrics.avgBatchProcessingTime = batchTime
            self.performanceMetrics.lastBatchSize = batchSize
            self.performanceMetrics.batchesProcessed += 1
        }
    }
    
    private func updateRecognitionStatistics() {
        DispatchQueue.main.async {
            let stats = self.recognitionStats
            
            self.addLog("ğŸ“Š è¯†åˆ«ç»Ÿè®¡: å¤„ç† \(stats.buffersProcessed) ç¼“å†²åŒº, ç«¯ç‚¹ \(stats.endpointsDetected), é¢„å¤„ç†é”™è¯¯ \(stats.preprocessingErrors)")
            
            // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performanceMetrics.totalBuffersProcessed = stats.buffersProcessed
            self.performanceMetrics.totalEndpointsDetected = stats.endpointsDetected
            
            // é‡ç½®ç»Ÿè®¡ï¼ˆæ¯æ¬¡æ›´æ–°åï¼‰
            self.recognitionStats = RecognitionStatistics()
        }
    }
    
    // MARK: - Helper Methods
    
    private func getTextFromResultSafely(_ result: UnsafePointer<SherpaOnnxOnlineRecognizerResult>) -> String {
        let textPointer = result.pointee.text
        
        guard let validTextPointer = textPointer else {
            return ""
        }
        
        let textLength = strlen(validTextPointer)
        guard textLength <= 10000 else {
            // æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            let truncatedData = Data(bytes: validTextPointer, count: min(Int(textLength), 10000))
            return String(data: truncatedData, encoding: .utf8) ?? ""
        }
        
        let resultString = String(cString: validTextPointer)
        return resultString.filter { $0.isPrint || $0.isWhitespace }
    }
    
    private func addLog(_ message: String) {
        let timestamp = DateFormatter.logFormatter.string(from: Date())
        let logMessage = "[\(timestamp)] \(message)"
        
        DispatchQueue.main.async {
            self.logs.append(logMessage)
            if self.logs.count > 100 {
                self.logs.removeFirst(self.logs.count - 100)
            }
        }
        
        print(logMessage)
    }
}

// MARK: - Supporting Components

/// éŸ³é¢‘é¢„å¤„ç†å™¨
class AudioPreprocessor {
    private let sampleRate: Double
    private let memoryManager: MemoryManager
    private let logger = os.Logger(subsystem: "com.capswriter", category: "AudioPreprocessor")
    
    init(sampleRate: Double, memoryManager: MemoryManager) {
        self.sampleRate = sampleRate
        self.memoryManager = memoryManager
    }
    
    func process(_ buffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
        // æ‰§è¡ŒéŸ³é¢‘é¢„å¤„ç†ï¼šé™å™ªã€å¢ç›Šæ§åˆ¶ç­‰
        // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„éŸ³é¢‘å¤„ç†é€»è¾‘
        
        // ç®€å•çš„éŸ³é‡æ ‡å‡†åŒ–
        guard let channelData = buffer.floatChannelData else { return buffer }
        
        let frameLength = Int(buffer.frameLength)
        let samples = channelData[0]
        
        // è®¡ç®—RMSå¹¶æ ‡å‡†åŒ–
        var rms: Float = 0.0
        vDSP_rmsqv(samples, 1, &rms, vDSP_Length(frameLength))
        
        if rms > 0.001 { // é¿å…é™éŸ³æ—¶çš„é™¤é›¶
            let targetRMS: Float = 0.1
            let gain = targetRMS / rms
            let clampedGain = min(max(gain, 0.1), 10.0) // é™åˆ¶å¢ç›ŠèŒƒå›´
            
            vDSP_vsmul(samples, 1, &clampedGain, samples, 1, vDSP_Length(frameLength))
        }
        
        return buffer
    }
}

/// è¯†åˆ«ç®¡é“
class RecognitionPipeline {
    private let batchSize: Int
    private let memoryManager: MemoryManager
    
    init(batchSize: Int, memoryManager: MemoryManager) {
        self.batchSize = batchSize
        self.memoryManager = memoryManager
    }
    
    func processBatch(_ buffers: [AVAudioPCMBuffer]) -> [String] {
        // æ‰¹å¤„ç†è¯†åˆ«ç»“æœ
        // è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ‰¹å¤„ç†é€»è¾‘
        return []
    }
}

/// ç»“æœåå¤„ç†å™¨
class ResultPostprocessor {
    func process(_ text: String) -> String {
        // ç»“æœåå¤„ç†ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–ç­‰
        return text.trimmingCharacters(in: .whitespacesAndNewlines)
            .replacingOccurrences(of: "  ", with: " ")
    }
}

/// æ¨¡å‹ç¼“å­˜
class ModelCache {
    private let maxSize: Int
    private var cache: [String: Data] = [:]
    
    init(maxSize: Int) {
        self.maxSize = maxSize
    }
    
    func store(key: String, data: Data) {
        if cache.count >= maxSize {
            // ç§»é™¤æœ€æ—§çš„ç¼“å­˜
            if let firstKey = cache.keys.first {
                cache.removeValue(forKey: firstKey)
            }
        }
        cache[key] = data
    }
    
    func retrieve(key: String) -> Data? {
        return cache[key]
    }
}

/// è¯†åˆ«æ€§èƒ½æŒ‡æ ‡
struct RecognitionPerformanceMetrics {
    var avgBatchProcessingTime: TimeInterval = 0.0
    var lastBatchSize: Int = 0
    var batchesProcessed: UInt64 = 0
    var totalBuffersProcessed: UInt64 = 0
    var totalEndpointsDetected: UInt64 = 0
}

/// è¯†åˆ«ç»Ÿè®¡ä¿¡æ¯
struct RecognitionStatistics {
    var buffersProcessed: UInt64 = 0
    var endpointsDetected: UInt64 = 0
    var preprocessingErrors: UInt64 = 0
}

// MARK: - Extensions

extension DateFormatter {
    static let logFormatter: DateFormatter = {
        let formatter = DateFormatter()
        formatter.dateFormat = "HH:mm:ss.SSS"
        return formatter
    }()
}